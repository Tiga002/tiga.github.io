---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Autonomous Lane Tracking RaceCar"
summary: "This project is my first project during my internship in SenseTime Corporation. It involved developing a 1:10 scale autonomous driving car capable of tracking double lane racetrack in real time and simple object detection. Solutions powered by OpenCV and Tensorflow had been adopted."

authors: ["Tiga Leung"]
tags: ["autonomous-driving", "robotics", "computer-vision"]
date: 2018-10-15

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

url_code: "https://github.com/Tiga002/Autonomous_RaceCar/tree/master/RaceCar_Client/catkin_ws/src/lane_detection"
url_video: "Lane_Detection_1.mp4"
url_pdf: ""
url_slides: ""

---
This project is my first project during my internship in SenseTime Corporation. It involved developing a 1:10 scale autonomous driving car capable of tracking double lane racetrack in real time and simple object detection. Solutions powered by OpenCV and Tensorflow had been adopted.
<img src="snap.jpg" height = "300" width="300"/>

## Hardware Design
The hardware design of our autonomous driving car is mainly inspired by a famous opensource project – “Cherry Autonomous Racecar – 1/10th scale Nvidia Jetson TX1 based RC car”. The vehicle is based on a 2 wheel-drive, 1:10 scale truck kit from Traxxas (Model #58024). It has a rear-wheel drivetrain and a single front steering. A powerful brushed DC motor and a servo motor are being used for throttle and steer respectively. The car can be controlled via remote control or PWM signals generated by a Teensy Microcontroller board which is added by me. For the computing part, Nvidia Jetson TX1 is the brain of the vehicle. It processes all the sensory inputs and issuing desire steering angle to the Teensy Microcontroller. For sensory setup, only a high resolution and FPS Web Cam is mounted on the vehicle. Robotic Operation System (ROS Kinetic) is used as the middleware, cooperating different input/output streams and algorithms with its topics and nodes features.

## Challenges
The size of our racetrack is relatively smaller compared to some similar projects like the  Audi Autonomous Driving Cup (AADC) or BARC Project by UC Berkeley. The adverse effect of the camera blind spot is larger. For example, when the vehicle is doing a U turn, only partial part of the lane is detected; double lane detected is not guaranteed. It requires our lane tracking algorithm as well as the controller have to be more robust and react much faster. In addition, compare to single-lane following, double lane following is relatively more difficult as the centre reference lane is missing. Real time computing a reference point for the lateral PID controller to track is the key for the task.

## System architecture
<img src="sw_system_arch.png" height = "600" width="600"/>

## Double Lane Tracking algorithm
<img src="double_lane_tracking_algorithm.png" height = "600" width="600"/>
